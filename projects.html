<!DOCTYPE html>
<!-- saved from url=(0046)http://www.cs.cmu.edu/afs/cs/user/gilwool/www/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Gilwoo Lee</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

    <!--link rel="stylesheet/less" href="less/bootstrap.less" type="text/css" /-->
    <!--link rel="stylesheet/less" href="less/responsive.less" type="text/css" /-->
    <!--script src="js/less-1.3.3.min.js"></script-->
    <!--append ‘#!watch’ to the browser URL, then refresh the page. -->

    <link href="./bootstrap.min.css" rel="stylesheet">
    <link href="./style.css" rel="stylesheet">

  <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
  <!--[if lt IE 9]>
    <script src="js/html5shiv.js"></script>
  <![endif]-->

  <!-- Fav and touch icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://www.cs.cmu.edu/afs/cs/user/gilwool/www/img/apple-touch-icon-144-precomposed.png">
  <link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://www.cs.cmu.edu/afs/cs/user/gilwool/www/img/apple-touch-icon-114-precomposed.png">
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://www.cs.cmu.edu/afs/cs/user/gilwool/www/img/apple-touch-icon-72-precomposed.png">
  <link rel="apple-touch-icon-precomposed" href="http://www.cs.cmu.edu/afs/cs/user/gilwool/www/img/apple-touch-icon-57-precomposed.png">
  <link rel="shortcut icon" href="http://www.cs.cmu.edu/afs/cs/user/gilwool/www/img/favicon.png">

    <script type="text/javascript" src="./jquery.min.js"></script>
    <script type="text/javascript" src="./bootstrap.min.js"></script>
    <script type="text/javascript" src="./scripts.js"></script>
</head>

<body style="">
<br>
<div class="container">
    <nav class="navbar navbar-default">
      <ul class="nav navbar-nav">
        <li><a href="./index.html">Home</a></li>
        <li><a href="./publications.html">Publications</a></li>
        <li class="active"><a href="#">Projects</a></li>
        <li><a href="./teaching.html">Teaching</a></li>
      </ul>
    </nav>

    <div class="row clearfix">
        <div class="col-md-2 column">
            <img alt="140x140" width="140" height="140" src="./gilwoo_sq.png" class="img-circle">
        </div>
        <div class="col-md-10 column">
            <div class="page-header" style="margin-top:12px">
                <h1>
                    Gilwoo Lee
                    <br>
                    <h2>
                    <small>CEO & Founder of Zordi. PhD in Robotics AI from Paul G. Allen School of Computer Science &amp; Engineering at the University of Washington
                    </small>
                    </h2>
                </h1>
            </div>
        </div>
    </div>
    <br>
    <div class="row clearfix">
        <div class="col-md-12 column">

            <h3 class="text-left">
                Projects
            </h3>
            <br>
            <ul>
                <li>
                    <h4>The Curious Minded Machine</h4>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/-AqMvXtW37Y" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <p>Curiosity is widely recognized as a fundamental mode of cognition and is particularly critical during childhood development. Developing intelligent robots with a sense of "curiosity" may lead to an important breakthrough in artificial intelligence: agents that proactively expand their knowledge and capabilities by themselves through information-generation. We propose a principled Bayesian reinforcement learning (BRL) framework that incorporates a mathematically elegant way to model curiosity. We show that BRL yields naturally curious behaviors that benefit long-term task performance. I have been the main research scientist for BRL algorithms. This project is collaboration among University of Washington, CMU, University of Pennsylvania, MIT, and Honda Research Institute USA.
                    </p>
                    <p>

                    Papers:
                    <a href="https://arxiv.org/abs/1810.01014">Bayesian Policy Optimization</a>, <a href="https://arxiv.org/pdf/2002.03042.pdf"> Bayesian Residual Policy Optimization </a><br>
                    Media appearance: <a href="https://www.geekwire.com/2018/curious-whether-robots-can-curious-university-washington-joins-initiative-find/">GeekWire</a>
                    </p>
                    <br>
                </li>
                <li>
                    <h4>Scalable, Adaptive, and Resilient Autonomy (SARA)</h4>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/QziFzm_Lvw4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <p>SARA is a research collaboration with U.S. Army Research Laboratory's Combat Capabilities Development Command (CCDC). We work on increasing the operational tempo and mobility of autonomous ground systems to traverse complex off-road environments. I am coordinating the University of Washington team and integrating a fast and optimal motion planning algorithm, <a href="https://github.com/personalrobotics/gls">Generalized Lazy Search</a>. We will be participating in a two-week experimentation event in Oct 2020, at Base Camp Lejeune in North Carolina.
                    </p>
                    <p>
                    <a href="https://www.army.mil/article/235978"> Press release </a>
                    </p>
                    <br/>
                </li>
                <li>
                    <h4>Food Manipulation for Assistive Feeding</h4>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/t2eO4CD-0WY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <p>Eating is an activity of daily living (ADL) and losing the ability to self-feed can be devastating. Eating free-form food is one of the most intricate manipulation tasks we perform everyday, demanding robust non-prehensile manipulation of a deformable hard-to-model target. Through this project, we develop algorithms and technologies towards a robotic system that can autonomously feed people with upper-extremity mobility limitations. I have co-implemented the system, designed and implemented bite-picking strategies. </p>
                    <p>
                    <a href="https://personalrobotics.cs.washington.edu/blog/food-manipulation/">Project page</a>
                    </p>
                    <br>
                </li>
                <li>
                    <p><b>MuSHR</b></p>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/N_ntS3ywYuM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <p>Our goal is to democratize robotics with MuSHR, the Multi-agent System for non-Holonomic Racing. We are developing a low-cost racecar platform with open-source software, tutorials, and class assignments. We want anyone from high school students to Ph.D. researchers to be able to learn about and perform research in robotics. Each MuSHR racecar is fitted with a full-suspension base, IMU, NVIDIA Jetson TX2, YDLIDAR laser scanner, and Intel RealSense RGBD camera. We have over 20 racecars in our fleet.
                    I have been mainly in charge of developing motion planners to be used in undergraduate coursework, setting up an optical motion capture system, and multi-car control.
                    </p>
                    <p><a href="https://mushr.io/"> Project page</a>
                    </p>
                    <br>

                </li>
                <li>
                    <h4><b>Learning Human Gestures and Synthesizing Real-time Finger Motions</b></h4>
                    <img src="./figs/handmotion.png" style="width:800px;">
                    <p>To work in uncertain environments with humans, robots must understand how humans behave and communicate. Human behavior involves the highly structured, multi-channel coordination of gaze, gesture, verbal and facial movements. To address the challenge of interpreting human gestures, we have collected a multimodal, wholistic dataset of people having social interaction captured with a motion capture system, which amounts to 16.2M frames. This is the largest dataset available. The statistical analysis verifies strong intraperson and interperson covariance of arm, hand, and speech features, potentially enabling new directions on data- driven social behavior analysis, prediction, and synthesis. We further demonstrate the use of this dataset to synthesize finger motions aligned with speech. I have been the lead developer on this project and the first author of the published ICCV 2019 paper.</p>
                    <p>
                    <a href="https://github.com/facebookresearch/TalkingWithHands32M">Github,</a> 
                    <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Talking_With_Hands_16.2M_A_Large-Scale_Dataset_of_Synchronized_Body-Finger_ICCV_2019_paper.pdf">Paper</a>
                    </p>
                    <br>
                </li>
                <li>
                    <h4><b>Human-Robot Handover</b></h4>
                    <img src="./figs/handover.png" style="width:800px;">
                    <p>Human-robot collaboration can enable humans to work on safe and less physically demanding tasks while robots take care of risky and heavy-payload tasks. In order for humans and robots to collaborate safely and swiftly, robots need to understand human intentions and act in a way that is predictable and legible from human perspective. We have proposed a unified generative model of human reaching motion that allows the robot to infer human intent and plan legible motions. Our study on human reaching motion reveals that elliptical motion model yields a good fit to empirical data. I have analyzed the collected user study data and implemented legible robot motions.
                    </p>
                    <p>
                        <a href="https://personalrobotics.cs.washington.edu/publications/   sheikholeslami2018reaching.pdf">Paper</a>
                    </p>
                    <br>
                </li>

                <li>
                    <h4><b>Multi-step Mobile Manipulation with Error-recovery</b></h4>
                    <figure>
                    <img src="./figs/magi.png" style="width:800px;">
                      <figcaption>HERB performing a table-clearing task via sequential motion planning</figcaption>
                    </figure>
                    <br>
                    <p>
                    Household manipulation presents a challenge to robots because it requires perceiving a variety of objects, planning multi-step motions, and recovering from failure. In collaboration with Toyota Motor Engineering & Manufacturing North America, we have worked on practical practical techniques that improve performance in these areas by considering the complete system in the context of this specific domain. We validate these techniques on a table-clearing task that involves loading objects into a tray and transporting it. The results show that these techniques improve success rate and task completion time by incorporating expected real-world performance into the system design. I was in charge of error detection and recovery and empirical evaluation of the overall system.
                    </p>
                    <p>
                    <a href="https://github.com/personalrobotics/prpy">Github,</a> 
                    <a href="./files/ISER2016.pdf" >Paper</a>
                    </p>
                    <br>

                </li>
                <li>
                    <p><b>AIKIDO</b></p>
                    <p>AIKIDO is a C++ library, complete with Python bindings, for solving robotic motion planning and decision making problems. This library is tightly integrated with DART for kinematic/dynamics calculations and OMPL for motion planning. AIKIDO optionally integrates with ROS for execution on real robots. AIKIDO is currently being used across multiple labs and projects including University of Washington, Carnegie Mellon University, University of Southern California, and Robotics Collaborative Technology Alliance (RCTA, sponsored by US Army Research Laboratory).
                    I am one of the lead developer and maintainer of the library.
                    </p>
                    <p><a href="https://github.com/personalrobotics/aikido">Github</a></p>
                    <br>
                </li>

                <li>
                    <p><b>HERB</b></p>
                    <img src="./figs/herb_wave.jpg" style="width:600px;">
                    <br>
                    <p>HERB, the Home Exploring Robot Butler, serves as the realistic testbed for all of our algorithms and as a focal point of our industry and academic collaborations. It is a bimanual mobile manipulator comprised of two Barrett WAM arms on a Neobotix base. It is equipped with a suite of image and range sensors, including the Carnegie Robotics MultiSense SL and Intel RealSense. I have been mainly in charge of designing the 3rd generation of HERB and maintaining its perception and control pipelines.
                    </p>
                    <p>
                    Media appearance: <a href="https://www.geekwire.com/2018/university-washingtons-herb-robot-makes-cameo-x-files-automated-sushi-waiter/">GeekWire</a>
                    <br>
                </li>
            </ul>

            <hr>

        </div>
    </div>
</div>


</body></html>




